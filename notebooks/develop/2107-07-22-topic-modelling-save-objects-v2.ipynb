{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk.tokenize as tk\n",
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from langdetect import DetectorFactory\n",
    "from langdetect import detect\n",
    "from datetime import datetime\n",
    "\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Cleaning and Preprocessing\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "additional_exclude_words = [u'beer', u'one', u'like', u'well', u'really', u'very', u'it\\x92s', \n",
    "                            u'it', u'beers',u'would',u'taste',u'tastes',u'get',u'i\\x92m',u'quite', u'i\\x92ve',\n",
    "                            u'bit',u'much',u'good',u'better',u'think',u'first',u'new',u'try', u'updated']\n",
    "stop |= set(additional_exclude_words)\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc, name_tokens):\n",
    "    #stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \" \".join([ch for ch in doc.lower().split() if ch not in exclude])\n",
    "    normalized = \" \".join([lemma.lemmatize(word) for word in punc_free.split()])\n",
    "    tokens = tk.word_tokenize(normalized)\n",
    "    tokens_with_pos = [x[0] for x in nltk.pos_tag(tokens) if x[1] in ('NN','JJ') and \n",
    "                       x[0] not in name_tokens and len(x[0])>2 and x[0] not in stop]\n",
    "    return tokens_with_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Establishing Connection\n",
    "\n",
    "creds = {'user': 'rthakur',\n",
    "         'password': 'brew',\n",
    "         'host': 'capstone-brewing.cyol5m7lekm6.us-east-1.rds.amazonaws.com',\n",
    "         'dbname': 'brew'}\n",
    "try:\n",
    "    conn = psycopg2.connect(\"dbname={} user={} password={} host={}\".format(creds['dbname'], creds['user'], \n",
    "                                                                           creds['password'], creds['host']))\n",
    "except:\n",
    "    print \"Unable to connect to the database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    create table wrk.rohan_test AS (select * from wrk.beer limit 50)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "no results to fetch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-39961a4f394a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m: no results to fetch"
     ]
    }
   ],
   "source": [
    "print cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-12 18:30:18.313000\n",
      "2017-08-12 23:17:52.105000\n"
     ]
    }
   ],
   "source": [
    "print str(datetime.now())\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    select b.id,\n",
    "       b.name,\n",
    "       r.text\n",
    "    from wrk.beer b left join wrk.review r on b.id = r.beer_id\n",
    "    \"\"\")\n",
    "rows = cur.fetchall()\n",
    "reviews_clean = []\n",
    "review_ids = []\n",
    "\n",
    "for row in rows:\n",
    "    if not row[2]:\n",
    "        continue\n",
    "    review = row[2].decode('utf-8')\n",
    "    name = row[1]\n",
    "    name_tokens = nltk.word_tokenize(name.lower())\n",
    "    try:\n",
    "        if detect(review)=='en':\n",
    "            rev_clean = clean(review.lower(), name_tokens)\n",
    "            reviews_clean.append(rev_clean)\n",
    "            review_ids.append(int(row[0]))\n",
    "    except:\n",
    "        continue\n",
    "dictionary = corpora.Dictionary(reviews_clean)\n",
    "\n",
    "print str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth\n"
     ]
    }
   ],
   "source": [
    "print reviews_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.save('../../data/beerwords_v5.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load('../../data/beerwords_v5.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2017-08-13 00:37:43.479000\n",
      "500\n",
      "2017-08-13 00:40:25.655000\n",
      "1000\n",
      "2017-08-13 00:43:36.273000\n",
      "1500\n",
      "2017-08-13 00:46:42.996000\n",
      "2000\n",
      "2017-08-13 00:49:19.020000\n",
      "2500\n",
      "2017-08-13 00:52:15.805000\n",
      "3000\n",
      "2017-08-13 00:54:36.842000\n",
      "3500\n",
      "2017-08-13 00:57:25.332000\n",
      "4000\n",
      "2017-08-13 00:59:28.665000\n",
      "4500\n",
      "2017-08-13 01:02:00.518000\n"
     ]
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "corpus = []\n",
    "corpus_indices = []\n",
    "revs_ids = zip(review_ids, reviews_clean)\n",
    "for x in range(4611):\n",
    "    if x%500==0:\n",
    "        print x\n",
    "        print str(datetime.now())\n",
    "    if x+1 not in review_ids:\n",
    "        continue\n",
    "    current_revs = [r[1] for r in revs_ids if r[0]==x+1]\n",
    "    revs_for_corpus = [item for sublist in current_revs for item in sublist]\n",
    "    #dict_temp = corpora.Dictionary(current_revs)\n",
    "    #dict_temp.save('../../data/dictionaries/dict_' + str(x) + '.dict')\n",
    "    corpus += [dictionary.doc2bow(revs_for_corpus)]\n",
    "    corpus_indices.append(x+1)\n",
    "    \n",
    "corpora.MmCorpus.serialize('../../data/beercorpus_v5.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../../data/beer_model_v5.expElogbeta.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f6550a9f4008>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMmCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../data/beercorpus_v5.mm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbeer_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../data/beer_model_v5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#print str(datetime.now())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#beer_model =  models.LdaModel(corpus, id2word=dictionary, num_topics=200)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print str(datetime.now())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rohan_000\\Anaconda2\\lib\\site-packages\\gensim\\models\\ldamodel.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, fname, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m         \"\"\"\n\u001b[0;32m   1048\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mmap'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mmap'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[0mstate_fname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.state'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rohan_000\\Anaconda2\\lib\\site-packages\\gensim\\utils.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, fname, mmap)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loaded %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rohan_000\\Anaconda2\\lib\\site-packages\\gensim\\utils.pyc\u001b[0m in \u001b[0;36m_load_specials\u001b[1;34m(self, fname, mmap, compress, subname)\u001b[0m\n\u001b[0;32m    296\u001b[0m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmmap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rohan_000\\Anaconda2\\lib\\site-packages\\numpy\\lib\\npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../../data/beer_model_v5.expElogbeta.npy'"
     ]
    }
   ],
   "source": [
    "corpus = corpora.MmCorpus('../../data/beercorpus_v5.mm')\n",
    "beer_model = models.LdaModel.load('../../data/beer_model_v5', mmap='r')\n",
    "#print str(datetime.now())\n",
    "#beer_model =  models.LdaModel(corpus, id2word=dictionary, num_topics=200)\n",
    "#print str(datetime.now())\n",
    "\n",
    "#beer_model.save('../../data/beer_model_v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2017-08-13 01:30:27.697000\n",
      "500\n",
      "2017-08-13 01:30:28.591000\n",
      "1000\n",
      "2017-08-13 01:30:29.777000\n",
      "1500\n",
      "2017-08-13 01:30:31.297000\n",
      "2000\n",
      "2017-08-13 01:30:33.187000\n",
      "2500\n",
      "2017-08-13 01:30:35.495000\n",
      "3000\n",
      "2017-08-13 01:30:37.222000\n",
      "3500\n",
      "2017-08-13 01:30:40.386000\n",
      "4000\n",
      "2017-08-13 01:30:43.536000\n",
      "4500\n",
      "2017-08-13 01:30:44.996000\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = []\n",
    "for x in range(4611):\n",
    "    if x%500==0:\n",
    "        print x\n",
    "        print str(datetime.now())\n",
    "    if x+1 not in review_ids:\n",
    "        continue\n",
    "    corpus_indices.append(x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-13 01:31:32.502000\n",
      "2017-08-13 01:36:04.399000\n"
     ]
    }
   ],
   "source": [
    "#Similarity\n",
    "print str(datetime.now())\n",
    "index = similarities.MatrixSimilarity(beer_model[corpus])\n",
    "print str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-13 01:39:37.427000\n",
      "2017-08-13 01:42:33.096000\n"
     ]
    }
   ],
   "source": [
    "#Similarity\n",
    "print str(datetime.now())\n",
    "df_dict = {'beer_id':[], 'similar_beer_id':[], 'similarity_score':[]}\n",
    "for i, corp in zip(corpus_indices, corpus):\n",
    "    b = beer_model[corp]\n",
    "    like_kernel = sorted(list(enumerate(index[b])), key = lambda x:x[1], reverse=True)[1:]\n",
    "    df_dict['beer_id'].extend([i]*len(like_kernel))\n",
    "    df_dict['similar_beer_id'].extend([tup[0] for tup in like_kernel])\n",
    "    df_dict['similarity_score'].extend([tup[1] for tup in like_kernel])\n",
    "print str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(data=df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(path_or_buf = '../../data/beer_similarities_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creaminess\n"
     ]
    }
   ],
   "source": [
    "print dictionary[572]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the very beginning: 2017-08-13 12:07:53.416000\n",
      "Imports complete: 2017-08-13 12:07:53.417000\n",
      "Loaded all files 2017-08-13 12:07:53.745000\n",
      "Actual function done: 2017-08-13 12:08:24.565000\n"
     ]
    }
   ],
   "source": [
    "print 'At the very beginning: ' + str(datetime.now())\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "print 'Imports complete: ' + str(datetime.now())\n",
    "\n",
    "dictionary = corpora.Dictionary.load('../../data/beerwords_v5.dict')\n",
    "corpus = corpora.MmCorpus('../../data/beercorpus_v5.mm')\n",
    "corpus_indices = pd.read_csv('../../data/correct_corpus_ids.csv')['corpus_ids'].tolist()\n",
    "\n",
    "print 'Loaded all files ' + str(datetime.now())\n",
    "\n",
    "def get_cloud_inputs(beer_id, dictionary, corpus, corpus_indices, num_words=200):\n",
    "    '''\n",
    "    beer_id: ID of the beer (from wrk.beer) for which you want the word cloud.\n",
    "    dictionary: type gensim.corpora.dictionary -> load this from saved files\n",
    "    corpus: type gensim.corpora.MMCorpus -> load this from saved files\n",
    "    corpus_indices: Mapping list for corpus to beer\n",
    "    num_words: Number of highest frequency words to be returned \n",
    "    '''\n",
    "    for i, corp in zip(corpus_indices, corpus):\n",
    "        if i==beer_id:\n",
    "            correct_corp = corp\n",
    "            break\n",
    "    sorted_corpus =  sorted(correct_corp, key = lambda x:x[1], reverse=True)[:num_words]\n",
    "    return [(dictionary[id_], count_) for id_, count_ in sorted_corpus]\n",
    "\n",
    "get_cloud_inputs(2345, dictionary, corpus, corpus_indices)\n",
    "\n",
    "print 'Actual function done: ' + str(datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_corpus_ids = pd.DataFrame(corpus_indices, columns=[\"corpus_ids\"])\n",
    "df_corpus_ids.to_csv(path_or_buf = '../../data/correct_corpus_ids.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
