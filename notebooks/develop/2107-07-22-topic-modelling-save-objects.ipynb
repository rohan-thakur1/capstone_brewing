{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohan_000\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk.tokenize as tk\n",
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from langdetect import DetectorFactory\n",
    "from langdetect import detect\n",
    "from datetime import datetime\n",
    "\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating beerwords dictionary from entire review corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Establishing Connection\n",
    "try:\n",
    "    conn = psycopg2.connect('''\n",
    "    dbname='brew' user='rthakur' \n",
    "    host='capstone-brewing.cyol5m7lekm6.us-east-1.rds.amazonaws.com' \n",
    "    password='brew'\n",
    "    ''')\n",
    "except:\n",
    "    print \"Unable to connect to the database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Cleaning and Preprocessing\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "additional_exclude_words = [u'beer', u'one', u'like', u'well', u'really', u'very', u'it\\x92s', \n",
    "                            u'it', u'beers',u'would',u'taste',u'tastes',u'get',u'i\\x92m',u'quite', u'i\\x92ve',\n",
    "                            u'bit',u'much',u'good',u'better',u'think',u'first',u'new',u'try', u'updated']\n",
    "stop |= set(additional_exclude_words)\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc, name_tokens):\n",
    "    #stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \" \".join([ch for ch in doc.lower().split() if ch not in exclude])\n",
    "    normalized = \" \".join([lemma.lemmatize(word) for word in punc_free.split()])\n",
    "    tokens = tk.word_tokenize(normalized)\n",
    "    tokens_with_pos = [x[0] for x in nltk.pos_tag(tokens) if x[1] in ('NN','JJ') and \n",
    "                       x[0] not in name_tokens and len(x[0])>2 and x[0] not in stop]\n",
    "    return tokens_with_pos\n",
    "#and wordnet.sysnet(term)\n",
    "#reviews_clean = [clean(r) for r in reviews]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-31 07:48:21.261000\n",
      "0\n",
      "2017-07-31 07:50:11.779000\n",
      "10000\n",
      "2017-07-31 07:53:58.229000\n",
      "20000\n",
      "2017-07-31 07:57:32.547000\n",
      "30000\n",
      "2017-07-31 08:01:26.506000\n",
      "40000\n",
      "2017-07-31 08:04:54.122000\n",
      "50000\n",
      "2017-07-31 08:08:46.196000\n",
      "60000\n",
      "2017-07-31 08:13:08.419000\n",
      "70000\n",
      "2017-07-31 08:17:00.284000\n",
      "80000\n",
      "2017-07-31 08:20:15.591000\n",
      "90000\n",
      "2017-07-31 08:23:47.684000\n",
      "100000\n",
      "2017-07-31 08:28:20.785000\n",
      "110000\n",
      "2017-07-31 08:39:01.466000\n",
      "120000\n",
      "2017-07-31 08:50:38.208000\n",
      "130000\n",
      "2017-07-31 09:00:52.565000\n",
      "140000\n",
      "2017-07-31 09:04:47.526000\n",
      "150000\n",
      "2017-07-31 09:08:36.249000\n",
      "160000\n",
      "2017-07-31 09:13:04.655000\n",
      "170000\n",
      "2017-07-31 09:17:43.891000\n",
      "180000\n",
      "2017-07-31 09:22:02.827000\n",
      "190000\n",
      "2017-07-31 09:25:57.747000\n",
      "200000\n",
      "2017-07-31 09:29:12.783000\n",
      "210000\n",
      "2017-07-31 09:32:35.909000\n",
      "220000\n",
      "2017-07-31 09:36:05.258000\n",
      "230000\n",
      "2017-07-31 09:39:44.548000\n",
      "240000\n",
      "2017-07-31 09:43:32.424000\n",
      "250000\n",
      "2017-07-31 09:47:45.868000\n",
      "260000\n",
      "2017-07-31 09:51:19.702000\n",
      "270000\n",
      "2017-07-31 09:54:46.511000\n",
      "280000\n",
      "2017-07-31 09:58:22.543000\n",
      "290000\n",
      "2017-07-31 10:02:49.290000\n",
      "300000\n",
      "2017-07-31 10:06:12.182000\n",
      "310000\n",
      "2017-07-31 10:10:01.479000\n",
      "320000\n",
      "2017-07-31 10:13:57.611000\n",
      "330000\n",
      "2017-07-31 10:17:33.871000\n",
      "340000\n",
      "2017-07-31 10:21:13.138000\n",
      "350000\n",
      "2017-07-31 10:24:44.468000\n",
      "360000\n",
      "2017-07-31 10:28:54.338000\n",
      "370000\n",
      "2017-07-31 10:33:26.604000\n",
      "380000\n",
      "2017-07-31 10:37:03.283000\n",
      "390000\n",
      "2017-07-31 10:41:04.447000\n",
      "400000\n",
      "2017-07-31 10:44:26.689000\n",
      "410000\n",
      "2017-07-31 10:48:05.057000\n",
      "420000\n",
      "2017-07-31 10:51:55.025000\n",
      "430000\n",
      "2017-07-31 10:55:35.123000\n",
      "440000\n",
      "2017-07-31 10:58:56.952000\n",
      "450000\n",
      "2017-07-31 11:02:34.623000\n",
      "460000\n",
      "2017-07-31 11:06:34.926000\n",
      "470000\n",
      "2017-07-31 11:10:01.837000\n",
      "480000\n",
      "2017-07-31 11:13:43.290000\n",
      "490000\n",
      "2017-07-31 11:18:43.103000\n",
      "500000\n",
      "2017-07-31 11:22:24.584000\n",
      "510000\n",
      "2017-07-31 11:26:21.798000\n",
      "520000\n",
      "2017-07-31 11:30:14.799000\n",
      "530000\n",
      "2017-07-31 11:34:10.624000\n",
      "540000\n",
      "2017-07-31 11:38:07.148000\n",
      "550000\n",
      "2017-07-31 11:42:19.767000\n",
      "560000\n",
      "2017-07-31 11:45:49.877000\n",
      "570000\n",
      "2017-07-31 11:49:34.976000\n",
      "580000\n",
      "2017-07-31 11:54:41.909000\n",
      "590000\n",
      "2017-07-31 11:58:51.750000\n",
      "600000\n",
      "2017-07-31 12:02:35.389000\n",
      "610000\n",
      "2017-07-31 12:06:05.142000\n",
      "620000\n",
      "2017-07-31 12:09:41.180000\n",
      "630000\n",
      "2017-07-31 12:13:05.865000\n",
      "640000\n",
      "2017-07-31 12:16:51.663000\n",
      "650000\n",
      "2017-07-31 12:20:46.712000\n",
      "660000\n",
      "2017-07-31 12:24:14.171000\n",
      "670000\n",
      "2017-07-31 12:27:32.814000\n",
      "680000\n",
      "2017-07-31 12:31:06.763000\n",
      "690000\n",
      "2017-07-31 12:34:18.433000\n",
      "700000\n",
      "2017-07-31 12:38:12.033000\n",
      "710000\n",
      "2017-07-31 12:41:52.964000\n",
      "720000\n",
      "2017-07-31 12:45:29.498000\n",
      "730000\n",
      "2017-07-31 12:49:22.026000\n",
      "740000\n",
      "2017-07-31 12:52:59.087000\n",
      "750000\n",
      "2017-07-31 12:58:24.686000\n",
      "760000\n",
      "2017-07-31 13:01:50.917000\n",
      "770000\n",
      "2017-07-31 13:04:49.098000\n",
      "780000\n",
      "2017-07-31 13:08:49.469000\n",
      "790000\n",
      "2017-07-31 13:12:57.848000\n",
      "800000\n",
      "2017-07-31 13:16:24.785000\n",
      "810000\n",
      "2017-07-31 13:20:42.646000\n",
      "820000\n",
      "2017-07-31 13:24:47.043000\n",
      "830000\n",
      "2017-07-31 13:28:20.340000\n",
      "840000\n",
      "2017-07-31 13:31:47.986000\n",
      "850000\n",
      "2017-07-31 13:35:28.487000\n",
      "860000\n",
      "2017-07-31 13:38:47.929000\n",
      "870000\n",
      "2017-07-31 13:42:23.285000\n",
      "880000\n",
      "2017-07-31 13:46:11.140000\n",
      "890000\n",
      "2017-07-31 13:50:10.252000\n",
      "900000\n",
      "2017-07-31 13:53:58.455000\n",
      "910000\n",
      "2017-07-31 13:57:41.138000\n",
      "920000\n",
      "2017-07-31 14:01:47.230000\n",
      "930000\n",
      "2017-07-31 14:04:45.326000\n",
      "940000\n",
      "2017-07-31 14:08:18.084000\n",
      "950000\n",
      "2017-07-31 14:11:55.257000\n",
      "960000\n",
      "2017-07-31 14:15:04.398000\n",
      "970000\n",
      "2017-07-31 14:18:22.711000\n",
      "980000\n",
      "2017-07-31 14:21:44.318000\n",
      "990000\n",
      "2017-07-31 14:25:00.430000\n",
      "1000000\n",
      "2017-07-31 14:28:26.610000\n",
      "1010000\n",
      "2017-07-31 14:31:54.146000\n",
      "1020000\n",
      "2017-07-31 14:35:11.776000\n",
      "1030000\n",
      "2017-07-31 14:38:41.669000\n",
      "1040000\n",
      "2017-07-31 14:42:02.048000\n",
      "2017-07-31 14:43:39.635000\n"
     ]
    }
   ],
   "source": [
    "##Compiling Documents\n",
    "print str(datetime.now())\n",
    "#dview = rc[:]\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "select b.id,\n",
    "       b.name,\n",
    "       r.text\n",
    "from wrk.beer b left join wrk.review r on b.id = r.beer_id\n",
    "\"\"\")\n",
    "rows = cur.fetchall()\n",
    "\n",
    "reviews_clean = []\n",
    "\n",
    "#def process_row(row):\n",
    "#    import nltk\n",
    "#    review = row[2].decode('utf-8')\n",
    "#    name = row[1]\n",
    "#    name_tokens = nltk.word_tokenize(name.lower())\n",
    " #   try:\n",
    "#        if detect(review)=='en':\n",
    "#            return clean(review.lower(), name_tokens)\n",
    "#    except:\n",
    "#        return\n",
    "\n",
    "#reviews_clean = dview.map_sync(process_row, rows)\n",
    "\n",
    "#for i, _ in enumerate(p.imap_unordered(do_work, xrange(num_tasks)), 1):\n",
    "#    sys.stderr.write('\\rdone {0:%}'.format(i/num_tasks))\n",
    "\n",
    "#for _ in tqdm.tqdm(pool.imap_unordered(do_work, tasks), total=len(tasks)):\n",
    "#    pass\n",
    "    \n",
    "i = 0\n",
    "for row in rows:\n",
    "    if i%10000==0:\n",
    "        print i\n",
    "        print str(datetime.now())\n",
    "    if not row[2]:\n",
    "        continue\n",
    "    review = row[2].decode('utf-8')\n",
    "    name = row[1]\n",
    "    name_tokens = nltk.word_tokenize(name.lower())\n",
    "    try:\n",
    "        if detect(review)=='en':\n",
    "            reviews_clean.append(clean(review.lower(), name_tokens))\n",
    "    except:\n",
    "        continue\n",
    "    i+=1\n",
    "print str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "982568\n"
     ]
    }
   ],
   "source": [
    "print len(reviews_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(reviews_clean)\n",
    "\n",
    "# Saving file\n",
    "dictionary.save('../../data/beerwords.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../../data/reviews_clean', 'wb') as fp:\n",
    "    pickle.dump(reviews_clean, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the larger beer model and corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Establishing Connection\n",
    "\n",
    "dictionary = corpora.Dictionary.load('../../data/beerwords.dict')\n",
    "try:\n",
    "    conn = psycopg2.connect('''\n",
    "    dbname='brew' user='rthakur' \n",
    "    host='capstone-brewing.cyol5m7lekm6.us-east-1.rds.amazonaws.com' \n",
    "    password='brew'\n",
    "    ''')\n",
    "except:\n",
    "    print \"Unable to connect to the database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2017-08-01 01:11:33.500000\n",
      "500\n",
      "2017-08-01 01:53:15.749000\n",
      "1000\n",
      "2017-08-01 02:31:47.596000\n",
      "1500\n",
      "2017-08-01 03:17:04.866000\n",
      "2000\n",
      "2017-08-01 03:45:19.887000\n",
      "2500\n",
      "2017-08-01 04:30:07.306000\n",
      "3000\n",
      "2017-08-01 05:03:27.176000\n",
      "3500\n",
      "2017-08-01 05:51:38.146000\n",
      "4000\n",
      "2017-08-01 06:22:06.722000\n",
      "4500\n",
      "2017-08-01 06:49:50.364000\n"
     ]
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "corpus = []\n",
    "\n",
    "for x in range(4611):\n",
    "    if x%500==0:\n",
    "        print x\n",
    "        print str(datetime.now())\n",
    "    cur.execute(\"\"\"\n",
    "    select b.id,\n",
    "       b.name,\n",
    "       r.text\n",
    "    from wrk.beer b left join wrk.review r on b.id = r.beer_id\n",
    "    where r.beer_id={0}\n",
    "    \"\"\".format(x+1))\n",
    "    rows = cur.fetchall()\n",
    "    reviews_clean = []\n",
    "    for row in rows:\n",
    "        review = row[2].decode('utf-8')\n",
    "        name = row[1]\n",
    "        name_tokens = nltk.word_tokenize(name.lower())\n",
    "        try:\n",
    "            if detect(review)=='en':\n",
    "                rev_clean = clean(review.lower(), name_tokens)\n",
    "                reviews_clean.append(u' '.join(rev_clean))\n",
    "        except:\n",
    "            continue\n",
    "    corpus += [dictionary.doc2bow(reviews_clean)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize('../../data/beercorpus.mm', corpus)\n",
    "#import pickle\n",
    "\n",
    "#with open('../../data/corpus', 'wb') as fp:\n",
    "#    pickle.dump(corpus, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beer_model =  models.LdaModel(corpus, id2word=dictionary, num_topics=200)\n",
    "#print corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beer_model.save('../../data/beer_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
