{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk.tokenize as tk\n",
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from langdetect import DetectorFactory\n",
    "from langdetect import detect\n",
    "from datetime import datetime\n",
    "\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Cleaning and Preprocessing\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "additional_exclude_words = [u'beer', u'one', u'like', u'well', u'really', u'very', u'it\\x92s', \n",
    "                            u'it', u'beers',u'would',u'taste',u'tastes',u'get',u'i\\x92m',u'quite', u'i\\x92ve',\n",
    "                            u'bit',u'much',u'good',u'better',u'think',u'first',u'new',u'try', u'updated']\n",
    "stop |= set(additional_exclude_words)\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc, name_tokens):\n",
    "    #stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \" \".join([ch for ch in doc.lower().split() if ch not in exclude])\n",
    "    normalized = \" \".join([lemma.lemmatize(word) for word in punc_free.split()])\n",
    "    tokens = tk.word_tokenize(normalized)\n",
    "    tokens_with_pos = [x[0] for x in nltk.pos_tag(tokens) if x[1] in ('NN','JJ') and \n",
    "                       x[0] not in name_tokens and len(x[0])>2 and x[0] not in stop]\n",
    "    return tokens_with_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Establishing Connection\n",
    "\n",
    "creds = {'user': 'rthakur',\n",
    "         'password': 'brew',\n",
    "         'host': 'capstone-brewing.cyol5m7lekm6.us-east-1.rds.amazonaws.com',\n",
    "         'dbname': 'brew'}\n",
    "try:\n",
    "    conn = psycopg2.connect(\"dbname={} user={} password={} host={}\".format(creds['dbname'], creds['user'], \n",
    "                                                                           creds['password'], creds['host']))\n",
    "except:\n",
    "    print \"Unable to connect to the database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    create table wrk.rohan_test AS (select * from wrk.beer limit 50)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "no results to fetch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-39961a4f394a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m: no results to fetch"
     ]
    }
   ],
   "source": [
    "print cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-12 18:30:18.313000\n",
      "2017-08-12 23:17:52.105000\n"
     ]
    }
   ],
   "source": [
    "print str(datetime.now())\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    select b.id,\n",
    "       b.name,\n",
    "       r.text\n",
    "    from wrk.beer b left join wrk.review r on b.id = r.beer_id\n",
    "    \"\"\")\n",
    "rows = cur.fetchall()\n",
    "reviews_clean = []\n",
    "review_ids = []\n",
    "\n",
    "for row in rows:\n",
    "    if not row[2]:\n",
    "        continue\n",
    "    review = row[2].decode('utf-8')\n",
    "    name = row[1]\n",
    "    name_tokens = nltk.word_tokenize(name.lower())\n",
    "    try:\n",
    "        if detect(review)=='en':\n",
    "            rev_clean = clean(review.lower(), name_tokens)\n",
    "            reviews_clean.append(rev_clean)\n",
    "            review_ids.append(int(row[0]))\n",
    "    except:\n",
    "        continue\n",
    "dictionary = corpora.Dictionary(reviews_clean)\n",
    "\n",
    "print str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth\n"
     ]
    }
   ],
   "source": [
    "print reviews_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.save('../../data/beerwords_v5.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load('../../data/beerwords_v4.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2017-08-13 00:37:43.479000\n",
      "500\n",
      "2017-08-13 00:40:25.655000\n",
      "1000\n",
      "2017-08-13 00:43:36.273000\n",
      "1500\n",
      "2017-08-13 00:46:42.996000\n",
      "2000\n",
      "2017-08-13 00:49:19.020000\n",
      "2500\n",
      "2017-08-13 00:52:15.805000\n",
      "3000\n",
      "2017-08-13 00:54:36.842000\n",
      "3500\n",
      "2017-08-13 00:57:25.332000\n",
      "4000\n",
      "2017-08-13 00:59:28.665000\n",
      "4500\n",
      "2017-08-13 01:02:00.518000\n"
     ]
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "corpus = []\n",
    "corpus_indices = []\n",
    "revs_ids = zip(review_ids, reviews_clean)\n",
    "for x in range(4611):\n",
    "    if x%500==0:\n",
    "        print x\n",
    "        print str(datetime.now())\n",
    "    if x+1 not in review_ids:\n",
    "        continue\n",
    "    current_revs = [r[1] for r in revs_ids if r[0]==x+1]\n",
    "    revs_for_corpus = [item for sublist in current_revs for item in sublist]\n",
    "    #dict_temp = corpora.Dictionary(current_revs)\n",
    "    #dict_temp.save('../../data/dictionaries/dict_' + str(x) + '.dict')\n",
    "    corpus += [dictionary.doc2bow(revs_for_corpus)]\n",
    "    corpus_indices.append(x+1)\n",
    "    \n",
    "corpora.MmCorpus.serialize('../../data/beercorpus_v5.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-13 01:04:19.115000\n",
      "2017-08-13 01:25:03.276000\n"
     ]
    }
   ],
   "source": [
    "#corpus = corpora.MmCorpus('../../data/beercorpus_v4.mm')\n",
    "#beer_model = models.LdaModel.load('../../data/beer_model_v4', mmap='r')\n",
    "print str(datetime.now())\n",
    "beer_model =  models.LdaModel(corpus, id2word=dictionary, num_topics=200)\n",
    "print str(datetime.now())\n",
    "\n",
    "beer_model.save('../../data/beer_model_v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2017-08-13 01:30:27.697000\n",
      "500\n",
      "2017-08-13 01:30:28.591000\n",
      "1000\n",
      "2017-08-13 01:30:29.777000\n",
      "1500\n",
      "2017-08-13 01:30:31.297000\n",
      "2000\n",
      "2017-08-13 01:30:33.187000\n",
      "2500\n",
      "2017-08-13 01:30:35.495000\n",
      "3000\n",
      "2017-08-13 01:30:37.222000\n",
      "3500\n",
      "2017-08-13 01:30:40.386000\n",
      "4000\n",
      "2017-08-13 01:30:43.536000\n",
      "4500\n",
      "2017-08-13 01:30:44.996000\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = []\n",
    "for x in range(4611):\n",
    "    if x%500==0:\n",
    "        print x\n",
    "        print str(datetime.now())\n",
    "    if x+1 not in review_ids:\n",
    "        continue\n",
    "    corpus_indices.append(x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-13 01:31:32.502000\n",
      "2017-08-13 01:36:04.399000\n"
     ]
    }
   ],
   "source": [
    "#Similarity\n",
    "print str(datetime.now())\n",
    "index = similarities.MatrixSimilarity(beer_model[corpus])\n",
    "print str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-13 01:39:37.427000\n",
      "2017-08-13 01:42:33.096000\n"
     ]
    }
   ],
   "source": [
    "#Similarity\n",
    "print str(datetime.now())\n",
    "df_dict = {'beer_id':[], 'similar_beer_id':[], 'similarity_score':[]}\n",
    "for i, corp in zip(corpus_indices, corpus):\n",
    "    b = beer_model[corp]\n",
    "    like_kernel = sorted(list(enumerate(index[b])), key = lambda x:x[1], reverse=True)[1:]\n",
    "    df_dict['beer_id'].extend([i]*len(like_kernel))\n",
    "    df_dict['similar_beer_id'].extend([tup[0] for tup in like_kernel])\n",
    "    df_dict['similarity_score'].extend([tup[1] for tup in like_kernel])\n",
    "print str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(data=df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(path_or_buf = '../../data/beer_similarities_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creaminess\n"
     ]
    }
   ],
   "source": [
    "print dictionary[572]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the very beginning: 2017-08-13 12:07:53.416000\n",
      "Imports complete: 2017-08-13 12:07:53.417000\n",
      "Loaded all files 2017-08-13 12:07:53.745000\n",
      "Actual function done: 2017-08-13 12:08:24.565000\n"
     ]
    }
   ],
   "source": [
    "print 'At the very beginning: ' + str(datetime.now())\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "print 'Imports complete: ' + str(datetime.now())\n",
    "\n",
    "dictionary = corpora.Dictionary.load('../../data/beerwords_v5.dict')\n",
    "corpus = corpora.MmCorpus('../../data/beercorpus_v5.mm')\n",
    "corpus_indices = pd.read_csv('../../data/correct_corpus_ids.csv')['corpus_ids'].tolist()\n",
    "\n",
    "print 'Loaded all files ' + str(datetime.now())\n",
    "\n",
    "def get_cloud_inputs(beer_id, dictionary, corpus, corpus_indices, num_words=200):\n",
    "    '''\n",
    "    beer_id: ID of the beer (from wrk.beer) for which you want the word cloud.\n",
    "    dictionary: type gensim.corpora.dictionary -> load this from saved files\n",
    "    corpus: type gensim.corpora.MMCorpus -> load this from saved files\n",
    "    corpus_indices: Mapping list for corpus to beer\n",
    "    num_words: Number of highest frequency words to be returned \n",
    "    '''\n",
    "    for i, corp in zip(corpus_indices, corpus):\n",
    "        if i==beer_id:\n",
    "            correct_corp = corp\n",
    "            break\n",
    "    sorted_corpus =  sorted(correct_corp, key = lambda x:x[1], reverse=True)[:num_words]\n",
    "    return [(dictionary[id_], count_) for id_, count_ in sorted_corpus]\n",
    "\n",
    "get_cloud_inputs(2345, dictionary, corpus, corpus_indices)\n",
    "\n",
    "print 'Actual function done: ' + str(datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_corpus_ids = pd.DataFrame(corpus_indices, columns=[\"corpus_ids\"])\n",
    "df_corpus_ids.to_csv(path_or_buf = '../../data/correct_corpus_ids.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
